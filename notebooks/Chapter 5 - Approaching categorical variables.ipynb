{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "\"Freezing\": 0,\n",
    "\"Warm\": 1, \n",
    "\"Cold\": 2, \n",
    "\"Boiling Hot\": 3, \n",
    "\"Hot\": 4,\n",
    "\"Lava Hot\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/1534965597.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"ord_2\"] = df.ord_2.map(mapping)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../input/cat_train.csv\") \n",
    "df.loc[:, \"ord_2\"] = df.ord_2.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    142726\n",
       "1.0    124239\n",
       "2.0     97822\n",
       "3.0     84790\n",
       "4.0     67508\n",
       "5.0     64840\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/3187219047.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read the data\n",
    "df = pd.read_csv(\"../input/cat_train.csv\") # fill NaN values in ord_2 column\n",
    "df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\") # initialize LabelEncoder\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "# fit label encoder and transform values on ord_2 column\n",
    "# P.S: do not use this directly. fit first, then transform\n",
    "df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    142726\n",
       "6    124239\n",
       "1     97822\n",
       "0     84790\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create our example feature matrix\n",
    "example = np.array( [\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1]\n",
    "] )\n",
    "# print size in bytes\n",
    "print(example.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# create our example feature matrix\n",
    "example = np.array( [\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1]\n",
    "] )\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(sparse_example.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "sparse_example.data.nbytes + sparse_example.indptr.nbytes + sparse_example.indices.nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 399949768\n",
      "Full size of sparse array: 599964656\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# number of rows\n",
    "n_rows = 10000\n",
    "# number of columns\n",
    "n_cols = 100000\n",
    "# create random binary matrix with only 5% values as 1s\n",
    "example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols)) # print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\") # convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "full_size = ( sparse_example.data.nbytes + sparse_example.indptr.nbytes + sparse_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 144\n",
      "Size of sparse array: 24\n",
      "Full size of sparse array: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# create binary matrix\n",
    "example = np.array([[0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\") # convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "full_size = ( sparse_example.data.nbytes + sparse_example.indptr.nbytes + sparse_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 8000000\n",
      "Full size of sparse array: 16000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# create random 1-d array with 1001 different categories (int)\n",
    "example = np.random.randint(1000, size=1000000)\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = False to get dense array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "# fit and transform data with dense one hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1)) # print size in bytes for dense array\n",
    "print(f\"Size of dense array: {ohe_example.nbytes}\")\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = True to get sparse array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=True)\n",
    "# fit and transform data with sparse one-hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1)) # print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {ohe_example.data.nbytes}\")\n",
    "full_size = (\n",
    "ohe_example.data.nbytes +\n",
    "ohe_example.indptr.nbytes + ohe_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.ord_2 == \"Boiling Hot\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "0     84790\n",
       "1     97822\n",
       "2    142726\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "6    124239\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          67508\n",
       "1         124239\n",
       "2         142726\n",
       "3          64840\n",
       "4          97822\n",
       "           ...  \n",
       "599995    142726\n",
       "599996     84790\n",
       "599997    142726\n",
       "599998    124239\n",
       "599999     84790\n",
       "Name: id, Length: 600000, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>0</td>\n",
       "      <td>15634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>1</td>\n",
       "      <td>17734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>2</td>\n",
       "      <td>26082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>3</td>\n",
       "      <td>12428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>4</td>\n",
       "      <td>11919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>5</td>\n",
       "      <td>3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>6</td>\n",
       "      <td>22774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>19477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Expert</td>\n",
       "      <td>1</td>\n",
       "      <td>22956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Expert</td>\n",
       "      <td>2</td>\n",
       "      <td>33249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Expert</td>\n",
       "      <td>3</td>\n",
       "      <td>15792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Expert</td>\n",
       "      <td>4</td>\n",
       "      <td>15078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Expert</td>\n",
       "      <td>5</td>\n",
       "      <td>4225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Expert</td>\n",
       "      <td>6</td>\n",
       "      <td>28900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>0</td>\n",
       "      <td>13623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>1</td>\n",
       "      <td>15464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>2</td>\n",
       "      <td>22818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>3</td>\n",
       "      <td>10805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>4</td>\n",
       "      <td>10363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>5</td>\n",
       "      <td>2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>6</td>\n",
       "      <td>19899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>10800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>12364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>18035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Master</td>\n",
       "      <td>4</td>\n",
       "      <td>8209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Master</td>\n",
       "      <td>5</td>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Master</td>\n",
       "      <td>6</td>\n",
       "      <td>15734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Novice</td>\n",
       "      <td>0</td>\n",
       "      <td>22718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>26271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Novice</td>\n",
       "      <td>2</td>\n",
       "      <td>38233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Novice</td>\n",
       "      <td>3</td>\n",
       "      <td>17850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Novice</td>\n",
       "      <td>4</td>\n",
       "      <td>17373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Novice</td>\n",
       "      <td>5</td>\n",
       "      <td>4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Novice</td>\n",
       "      <td>6</td>\n",
       "      <td>33263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ord_1  ord_2  count\n",
       "0   Contributor      0  15634\n",
       "1   Contributor      1  17734\n",
       "2   Contributor      2  26082\n",
       "3   Contributor      3  12428\n",
       "4   Contributor      4  11919\n",
       "5   Contributor      5   3250\n",
       "6   Contributor      6  22774\n",
       "7        Expert      0  19477\n",
       "8        Expert      1  22956\n",
       "9        Expert      2  33249\n",
       "10       Expert      3  15792\n",
       "11       Expert      4  15078\n",
       "12       Expert      5   4225\n",
       "13       Expert      6  28900\n",
       "14  Grandmaster      0  13623\n",
       "15  Grandmaster      1  15464\n",
       "16  Grandmaster      2  22818\n",
       "17  Grandmaster      3  10805\n",
       "18  Grandmaster      4  10363\n",
       "19  Grandmaster      5   2894\n",
       "20  Grandmaster      6  19899\n",
       "21       Master      0  10800\n",
       "22       Master      1  12364\n",
       "23       Master      2  18035\n",
       "24       Master      3   8594\n",
       "25       Master      4   8209\n",
       "26       Master      5   2262\n",
       "27       Master      6  15734\n",
       "28       Novice      0  22718\n",
       "29       Novice      1  26271\n",
       "30       Novice      2  38233\n",
       "31       Novice      3  17850\n",
       "32       Novice      4  17373\n",
       "33       Novice      5   4889\n",
       "34       Novice      6  33263"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby( [\n",
    "\"ord_1\",\n",
    "\"ord_2\"\n",
    "] )[\"id\"].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_feature\"] = ( df.ord_1.astype(str)\n",
    "+ \"_\"\n",
    "+ df.ord_2.astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Contributor_3\n",
       "1         Grandmaster_6\n",
       "2                 nan_2\n",
       "3              Novice_4\n",
       "4         Grandmaster_1\n",
       "              ...      \n",
       "599995         Novice_2\n",
       "599996         Novice_0\n",
       "599997    Contributor_2\n",
       "599998         Master_6\n",
       "599999    Contributor_0\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_feature\"] = (\n",
    " df.ord_1.astype(str)\n",
    " + \"_\"\n",
    " + df.ord_2.astype(str)\n",
    " + \"_\"\n",
    " + df.ord_3.astype(str)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Contributor_3_c\n",
       "1         Grandmaster_6_e\n",
       "2                 nan_2_n\n",
       "3              Novice_4_a\n",
       "4         Grandmaster_1_h\n",
       "               ...       \n",
       "599995         Novice_2_a\n",
       "599996         Novice_0_n\n",
       "599997    Contributor_2_n\n",
       "599998         Master_6_m\n",
       "599999    Contributor_0_b\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    142726\n",
       "6    124239\n",
       "1     97822\n",
       "0     84790\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/4037129469.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read training data\n",
    "train = pd.read_csv(\"../input/cat_train.csv\") #read test data\n",
    "test = pd.read_csv(\"../input/cat_test.csv\")\n",
    "# create a fake target column for test data # since this column doesn't exist \n",
    "test.loc[:, \"target\"] = -1\n",
    "# concatenate both training and test data\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "# make a list of features we are interested in\n",
    "# id and target is something we should not encode\n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
    "# loop over the features list\n",
    "for feat in features:\n",
    "    # create a new instance of LabelEncoder for each feature \n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    # note the trick here\n",
    "    # since its categorical data, we fillna with a string\n",
    "    # and we convert all the data to string type\n",
    "    # so, no matter its int or float, its converted to string # int/float but categorical!!!\n",
    "    temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
    "    # we can use fit_transform here as we do not\n",
    "    # have any extra test data that we need to\n",
    "    # transform on separately\n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    "# split the training and test data again\n",
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "G        3404\n",
       "V        3107\n",
       "J        1950\n",
       "L        1657\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ord_4 = df.ord_4.fillna(\"NONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000, \"ord_4\"\n",
    "] = \"RARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "RARE     3607\n",
       "G        3404\n",
       "V        3107\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py\n",
    "# import pandas and model_selection module of scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn import model_selection \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read training data\n",
    "    df = pd.read_csv(\"../input/cat_train.csv\")\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # fetch labels\n",
    "    y = df.target.values\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)): df.loc[v_, 'kfold'] = f\n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"../input/cat_train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    120000\n",
       "1    120000\n",
       "2    120000\n",
       "3    120000\n",
       "4    120000\n",
       "Name: kfold, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "df.kfold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "599995    4\n",
       "599996    4\n",
       "599997    4\n",
       "599998    4\n",
       "599999    4\n",
       "Name: kfold, Length: 600000, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==0].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==1].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df[df.kfold==4].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_logres.py\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnâ€™t matter because all are categories \n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "    [df_train[features], df_valid[features]], axis=0\n",
    "    ) \n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.7861251326159475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 1, AUC = 0.7846381615622908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2, AUC = 0.7889849312597597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 3, AUC = 0.7848089535699552\n",
      "Fold = 4, AUC = 0.7861445704955669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") # now its time to label encode the features\n",
    "    for col in features:\n",
    "    # initialize LabelEncoder for each feature column\n",
    "\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) # get training data\n",
    "    x_train = df_train[features].values\n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    # initialize random forest model\n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1) # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter 5 - Approaching categorical variables.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold_ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     run(fold_)\n",
      "\u001b[1;32m/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter 5 - Approaching categorical variables.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# fill all NaN values with NONE\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# note that I am converting all columns to \"strings\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# it doesnt matter because all are categories \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m features:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df[df\u001b[39m.\u001b[39;49mcolumns[col]] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39mNONE\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# now its time to label encode the features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# initialize LabelEncoder for each feature column\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bigcube/Desktop/repos/AAAMLP/notebooks/Chapter%205%20-%20Approaching%20categorical%20variables.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     lbl \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mLabelEncoder()\n",
      "File \u001b[0;32m~/micromamba/envs/fastai/lib/python3.10/site-packages/pandas/core/indexes/base.py:5339\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5336\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5337\u001b[0m         key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m-> 5339\u001b[0m result \u001b[39m=\u001b[39m getitem(key)\n\u001b[1;32m   5340\u001b[0m \u001b[39m# Because we ruled out integer above, we always get an arraylike here\u001b[39;00m\n\u001b[1;32m   5341\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    24720\n",
       ">50K      7841\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../input/adult.csv\")\n",
    "\n",
    "df.income.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education.num      marital.status  \\\n",
       "0       90         ?   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "2       66         ?  186061  Some-college             10             Widowed   \n",
       "3       54   Private  140359       7th-8th              4            Divorced   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32558   40   Private  154374       HS-grad              9  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "              occupation   relationship   race     sex  capital.gain  \\\n",
       "0                      ?  Not-in-family  White  Female             0   \n",
       "1        Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                      ?      Unmarried  Black  Female             0   \n",
       "3      Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4         Prof-specialty      Own-child  White  Female             0   \n",
       "...                  ...            ...    ...     ...           ...   \n",
       "32556    Protective-serv  Not-in-family  White    Male             0   \n",
       "32557       Tech-support           Wife  White  Female             0   \n",
       "32558  Machine-op-inspct        Husband  White    Male             0   \n",
       "32559       Adm-clerical      Unmarried  White  Female             0   \n",
       "32560       Adm-clerical      Own-child  White    Male             0   \n",
       "\n",
       "       capital.loss  hours.per.week native.country income  \n",
       "0              4356              40  United-States  <=50K  \n",
       "1              4356              18  United-States  <=50K  \n",
       "2              4356              40  United-States  <=50K  \n",
       "3              3900              40  United-States  <=50K  \n",
       "4              3900              40  United-States  <=50K  \n",
       "...             ...             ...            ...    ...  \n",
       "32556             0              40  United-States  <=50K  \n",
       "32557             0              38  United-States  <=50K  \n",
       "32558             0              40  United-States   >50K  \n",
       "32559             0              40  United-States  <=50K  \n",
       "32560             0              20  United-States  <=50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/999898374.py:27: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/999898374.py:27: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.879492444118551\n",
      "Fold = 1, AUC = 0.8876200434664157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/999898374.py:27: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2, AUC = 0.8852609687685753\n",
      "Fold = 3, AUC = 0.8681269762152435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/999898374.py:27: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/folders/v5/dptls5vs3nnd8pxb3thc1h9r0000gn/T/ipykernel_25125/999898374.py:27: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 4, AUC = 0.8728581541840037\n",
      "----0.9034171104431152 seconds----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bigcube/micromamba/envs/fastai/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    # drop numerical columns\n",
    "    df = df.drop(num_cols, axis=1)\n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    # all columns are features except income and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]],\n",
    "        axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    print(f\"----{end_time} seconds----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
